{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning - Building a Deep Neural Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, activation, optimization, drop_prob=0.0, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        layer_dimensions: (list) number of nodes in each layer\n",
    "        activation: choice of activation functions ~ implemented relu, leaky relu, swish\n",
    "        drop_prob: drop probability for dropout layers\n",
    "        reg_lambda: regularization parameter\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.num_layers = len(layer_dimensions)\n",
    "        self.layer_dimensions = layer_dimensions\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimization\n",
    "        self.train_cost = []\n",
    "        self.val_cost = []\n",
    "        self.train_accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        \n",
    "        # init parameters        \n",
    "        model = {}\n",
    "        \n",
    "        for i in range(len(layer_dimensions) - 1):\n",
    "            layer_ahead = layer_dimensions[i+1]\n",
    "            layer_behind = layer_dimensions[i]\n",
    "            model['W'+str(i+1)] = np.random.randn(layer_ahead, layer_behind) * np.sqrt(2.0/(layer_ahead+layer_behind))\n",
    "            model['b'+str(i+1)] = np.zeros((layer_ahead,1))\n",
    "            \n",
    "        self.parameters = model\n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        W: Weight Matrix\n",
    "        b: bias\n",
    "        returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "        samples = A.shape[1]\n",
    "        features = A.shape[0]\n",
    "        \n",
    "        Z = np.dot(W,A) + b\n",
    "        return Z\n",
    "\n",
    "    def activationForward(self, A):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        A: input to the activation function\n",
    "        returns: activation(A)\n",
    "        \"\"\" \n",
    "        if self.activation == \"relu\":\n",
    "            A = self.relu(A)\n",
    "        elif self.activation == \"lrelu\":\n",
    "            A = self.leaky_relu(A,0.1)\n",
    "        elif self.activation == \"swish\":\n",
    "            A = self.swish(A)\n",
    "        return A\n",
    "\n",
    "\n",
    "    def relu(self, X):\n",
    "        \"\"\"ReLU activation unit as proposed in the Krizhevsky et al. 2012\"\"\"\n",
    "        activation = np.maximum(0,X)\n",
    "        return activation\n",
    "    \n",
    "    def leaky_relu(self, X, alpha):\n",
    "        \"\"\"Leaky ReLU activation unit.\n",
    "           alpha: the hyperparameter that replaces 0 in the ReLU\"\"\"\n",
    "        x = X.copy()\n",
    "        out = self.relu(x) - alpha * self.relu(-x)\n",
    "        return out\n",
    "        \n",
    "    def swish(self, X):\n",
    "        \"\"\"Swish activation unit as proposed by Ramachandran et al\n",
    "           https://arxiv.org/pdf/1710.05941.pdf\"\"\"\n",
    "        \n",
    "        x = X.copy()\n",
    "        out = x * self.sigmoid(x)\n",
    "        return out\n",
    "        \n",
    "    def sigmoid(self, X):\n",
    "        \"\"\"Sigmoid activation unit\"\"\"\n",
    "        x = X.copy()\n",
    "        out = 1/(1 + np.exp(-x))\n",
    "        return out\n",
    "            \n",
    "    def softmax(self, X):\n",
    "        \"\"\"Numerically Stable Softmax activation for the last layer of the Neural Network\"\"\"\n",
    "        S = X - np.amax(X, axis=0)\n",
    "        S = np.exp(S)\n",
    "        total = np.sum(S,axis=0, keepdims=True)\n",
    "        soft_m = S/total\n",
    "        \n",
    "        return soft_m\n",
    "    \n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        A: Activation\n",
    "        prob: dropout prob\n",
    "        returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        M = np.random.rand(A.shape[0], A.shape[1])\n",
    "        M = (M >= prob)*1.0\n",
    "        M /= (1- prob)\n",
    "        A *= M\n",
    "        return A, M\n",
    "\n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        cache = {} #Dictionary Data Structure used to store the relevant values that will be used in further computations\n",
    "        cache['A' + str(0)] = X\n",
    "        ls = self.num_layers\n",
    "        \n",
    "        for i in range(ls - 2):\n",
    "            W = self.parameters['W' + str(i+1)]\n",
    "            b = self.parameters['b' + str(i+1)]\n",
    "            A = cache['A' + str(i)]\n",
    "            Z = self.affineForward(A, W, b)\n",
    "            A_next = self.activationForward(Z)\n",
    "            \n",
    "            if self.drop_prob > 0:\n",
    "                A_next, M = self.dropout(A_next, self.drop_prob)\n",
    "                cache['M' + str(i+1)] = M\n",
    "            \n",
    "            cache['A'+str(i+1)] = A_next\n",
    "            cache['Z'+str(i+1)] = Z\n",
    "        \n",
    "        W = self.parameters['W'+str(ls-1)]\n",
    "        b = self.parameters['b'+str(ls-1)]\n",
    "        A_class = cache['A'+str(ls-2)]\n",
    "        \n",
    "        AL = self.affineForward(A_class, W, b) #without softmax. softmax will be applied while calculating the cost function\n",
    "        \n",
    "        return AL, cache\n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        AL: Activation of last layer, shape (num_classes, S)\n",
    "        y: labels, shape (S)\n",
    "        returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "        # compute loss\n",
    "        AL = self.softmax(AL)\n",
    "        \n",
    "        true_labels = one_hot(y)\n",
    "\n",
    "        y_hat = AL[y, range(y.shape[0])]\n",
    "        #y_hat[y_hat == 0] = 10**-10\n",
    "        #Computing the cross entropy loss\n",
    "        cost = -np.sum(np.log(y_hat))/ AL.shape[1] \n",
    "        \n",
    "        if self.reg_lambda > 0:\n",
    "            # add regularization\n",
    "            W_1, W_2, W_3 = self.parameters['W1'], self.parameters['W2'], self.parameters['W3']\n",
    "            reg_loss = (0.5 * self.reg_lambda * np.sum(W_1*W_1)) + (0.5 * self.reg_lambda * np.sum(W_2*W_2)) + (0.5 * self.reg_lambda * np.sum(W_3*W_3))\n",
    "            cost = cost + reg_loss\n",
    "         \n",
    "        dAL = AL - true_labels\n",
    "        return cost, dAL\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache, layer):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        dA_prev: gradient from the next layer.\n",
    "        cache: cache returned in affineForward\n",
    "        returns dA: gradient on the input to this layer\n",
    "                dW: gradient on the weights\n",
    "                db: gradient on the bias\n",
    "        \"\"\"\n",
    "        A = cache['A'+str(layer-1)]\n",
    "        W = self.parameters['W'+str(layer)]\n",
    "        samples = dA_prev.shape[1]\n",
    "        \n",
    "        if self.drop_prob > 0:\n",
    "            dA_prev = self.dropout_backward(dA_prev, cache, layer)\n",
    "        \n",
    "        dZ = self.activationBackward(dA_prev, cache, layer, \"lrelu\")\n",
    "        dW = 1/samples * np.dot(dZ, A.T)\n",
    "        db = 1/samples * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dA = np.dot(W.T, dZ)\n",
    "        \n",
    "        return dA, dW, db\n",
    "\n",
    "    def activationBackward(self, dA, cache, layer, activation):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        Z = cache['Z'+str(layer)]\n",
    "        if activation == \"relu\":\n",
    "            dx = self.relu_derivative(dA, Z)\n",
    "        elif activation == \"lrelu\":\n",
    "            dx = self.lrelu_derivative(dA, Z, 0.1)\n",
    "        elif activation == \"swish\":\n",
    "            dx = self.swish_derivative(dA, Z)\n",
    "        return dx\n",
    "    \n",
    "    def relu_derivative(self, dx, cached_x):\n",
    "        \"\"\"Derivative of ReLU activation used during Backpropagation\"\"\"\n",
    "        dA = dx * 1 * (cached_x > 0)\n",
    "        return dA\n",
    "\n",
    "    def lrelu_derivative(self, dx, cached_x, alpha):\n",
    "        \"\"\"Derivative of Leaky ReLU activation used during Backpropagation\"\"\"\n",
    "        dA = np.ones_like(cached_x)\n",
    "        dA[cached_x < 0] = alpha\n",
    "        return dA*dx\n",
    "    \n",
    "    def swish_derivative(self, dx, cached_x, beta):\n",
    "        \"\"\"Derivative of Swish activation used during Backpropagation\"\"\"\n",
    "        sig = self.sigmoid(dx)\n",
    "        return sig + (dx*sig*(1-sig))\n",
    "    \n",
    "    def dropout_backward(self, dA, cache, layer):\n",
    "        \"\"\"Derivative of Dropout used during Backpropagation\"\"\"\n",
    "        m= cache['M' + str(layer)]\n",
    "        return np.multiply(m, dA) / (1 - self.drop_prob)\n",
    "\n",
    "    def backPropagation(self, dAL, AL, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        Y: labels\n",
    "        cache: cached values during forwardprop\n",
    "        returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        \n",
    "\n",
    "        samples = AL.shape[1]\n",
    "        features = AL.shape[0]\n",
    "        layers = self.num_layers\n",
    "        \n",
    "        #computing gradients using error at the output, bias at the output, and the weight matrix immediately before\n",
    "        A = cache['A' + str(layers - 2)] # Activations at the layer previous layer\n",
    "        W = self.parameters['W' + str(layers - 1)]\n",
    "        dZ = dAL\n",
    "        dW = 1/samples * np.dot(dZ, A.T)\n",
    "        db = 1/samples * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dA = np.dot(W.T, dZ)\n",
    "        \n",
    "        gradients['dW' + str(self.num_layers - 1)] = dW\n",
    "        gradients['db' + str(self.num_layers - 1)] = db\n",
    "        \n",
    "        \n",
    "        for layer in range(layers-2,0,-1):\n",
    "            dA, dW, db = self.affineBackward(dA, cache, layer)\n",
    "            gradients['dW' + str(layer)] = dW\n",
    "            gradients['db' + str(layer)] = db\n",
    "            \n",
    "            \n",
    "           \n",
    "        if self.reg_lambda > 0:\n",
    "            # add gradients from L2 regularization to each dW\n",
    "            for i in range(self.num_layers - 2):\n",
    "                dW = gradients['dW' + str(i + 1)]\n",
    "                W = self.parameters['W' + str(i + 1)]\n",
    "                dW_updated = dW + self.reg_lambda /samples * W\n",
    "                gradients['dW' + str(i + 1)] = dW_updated\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def updateParameters(self, gradients, alpha, iteration, Optim_Dict):\n",
    "        \"\"\"\n",
    "        Updates the parameters according to the Optimization algorithm specified\n",
    "        gradients: gradients for each weight/bias\n",
    "        alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "        if self.optimizer == \"Adam\":\n",
    "            #Hyperparameters for Adam Optimization ~ beta1 and beta2 corresponds to the decay rates and eps ensure division is not by zero\n",
    "            beta1, beta2, eps = 0.9, 0.999, 1e-8\n",
    "\n",
    "            for l in range(1, self.num_layers):\n",
    "                Optim_Dict[\"Vdw\"+str(l)] = (beta1 * Optim_Dict[\"Vdw\"+str(l)]) + ((1 - beta1) * gradients[\"dW\" + str(l)])\n",
    "                Optim_Dict[\"Vdb\"+str(l)] = (beta1 * Optim_Dict[\"Vdb\"+str(l)]) + ((1 - beta1) * gradients[\"db\" + str(l)])\n",
    "                Optim_Dict[\"Sdw\"+str(l)] = (beta2 * Optim_Dict[\"Sdw\"+str(l)]) + ((1 - beta2) * (gradients[\"dW\" + str(l)] * gradients[\"dW\" + str(l)]))\n",
    "                Optim_Dict[\"Sdb\"+str(l)] = (beta2 * Optim_Dict[\"Sdb\"+str(l)]) + ((1 - beta2) * (gradients[\"db\" + str(l)] * gradients[\"db\" + str(l)]))\n",
    "\n",
    "                Vdw_corr = (Optim_Dict[\"Vdw\"+str(l)])/(1 - (beta1**(iteration+1)))\n",
    "                Vdb_corr = (Optim_Dict[\"Vdb\"+str(l)])/(1 - (beta1**(iteration+1)))\n",
    "                Sdw_corr = (Optim_Dict[\"Sdw\"+str(l)])/(1 - (beta2**(iteration+1)))\n",
    "                Sdb_corr = (Optim_Dict[\"Sdb\"+str(l)])/(1 - (beta2**(iteration+1)))\n",
    "\n",
    "                self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - alpha * (Vdw_corr/(np.sqrt(Sdw_corr) + eps))\n",
    "                self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - alpha * (Vdb_corr/(np.sqrt(Sdb_corr) + eps))\n",
    "\n",
    "        elif self.optimizer == \"SGD\":\n",
    "            for l in range(1, self.num_layers):\n",
    "                self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - alpha * gradients[\"dW\" + str(l)]\n",
    "                self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - alpha * gradients[\"db\" + str(l)]\n",
    "        \n",
    "    def train(self, X, y, iters=1000, alpha=0.001, batch_size=100, print_every=100):\n",
    "        \"\"\"\n",
    "        X: input samples, each column is a sample\n",
    "        y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        iters: number of training iterations\n",
    "        alpha: step size for gradient descent\n",
    "        batch_size: number of samples in a minibatch\n",
    "        print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "        validation_size = int(X.shape[1]*.1)\n",
    "        total_size = X.shape[1]\n",
    "        validation_X = X[:,total_size - validation_size:]\n",
    "        validation_y = y[total_size - validation_size:]\n",
    "        \n",
    "        X = X[:,0:total_size - validation_size]\n",
    "        y = y[0:total_size - validation_size]\n",
    "        \n",
    "        shapes = []\n",
    "        for j in range(len(self.layer_dimensions) - 1):\n",
    "            shapes.append((self.layer_dimensions[j], self.layer_dimensions[j+1]))\n",
    "        \n",
    "        #initializing momentum and ubiases for advanced optimization techniques\n",
    "        Optim_Dict = {}\n",
    "        if self.optimizer != \"SGD\":\n",
    "            for l in range(0, len(shapes)):\n",
    "                layer_ahead = shapes[l][1]\n",
    "                layer_behind = shapes[l][0]\n",
    "\n",
    "                Optim_Dict[\"Vdw\" + str(l+1)] = np.zeros((layer_ahead, layer_behind))\n",
    "                Optim_Dict[\"Vdb\" + str(l+1)] = np.zeros((layer_ahead, 1))\n",
    "                Optim_Dict[\"Sdw\" + str(l+1)] = np.zeros((layer_ahead, layer_behind))\n",
    "                Optim_Dict[\"Sdb\" + str(l+1)] = np.zeros((layer_ahead, 1))\n",
    "        \n",
    "        for i in range(0, iters):\n",
    "            # get minibatch\n",
    "            X_batch, y_batch = self.get_batch(X, y, batch_size)\n",
    "            \n",
    "            # forward prop\n",
    "            AL, cache = self.forwardPropagation(X_batch)\n",
    "            \n",
    "            # compute loss\n",
    "            cost, dAL = self.costFunction(AL, y_batch)\n",
    "\n",
    "            # compute gradients\n",
    "            gradients = self.backPropagation(dAL, AL, cache)\n",
    "\n",
    "            # update weights and biases based on gradient\n",
    "            self.updateParameters(gradients, alpha, i, Optim_Dict)\n",
    "            \n",
    "            if i % print_every == 0:\n",
    "                t_pred = np.argmax(AL, axis=0)\n",
    "                train_pred = np.mean(t_pred == y_batch)*100\n",
    "                self.train_accuracy.append(train_pred)\n",
    "                self.train_cost.append(cost)\n",
    "                print(\"Training Set Accuracy after,\",i,\"iteration = \", train_pred,\"%\") \n",
    "                print(\"Training Set Cost after,\",i,\"iteration = \",cost)\n",
    "                \n",
    "                v_pred = self.predict(validation_X)\n",
    "                val_pred = np.mean(v_pred == validation_y)*100\n",
    "                self.val_accuracy.append(val_pred)\n",
    "                print(\"Validation Set Accuracy after,\",i,\"iteration = \", val_pred, \"%\")\n",
    "                print()\n",
    "\n",
    "                \n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        AL, _ = self.forwardPropagation(X)\n",
    "        y_pred = np.argmax(AL, axis=0)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        X, y: samples and corresponding labels\n",
    "        batch_size: minibatch size\n",
    "        returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "        idx = np.random.choice(X.shape[1], batch_size, replace=True)\n",
    "        X_batch = X[:,idx]\n",
    "        y_batch = y[idx]\n",
    "\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS TO GET THE DATA AND PREPROCESS IT\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    folder: path to data folder\n",
    "    label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    One-hot encoding converts categorical labels to binary values\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((num_classes, y.shape[0]))\n",
    "    y_one_hot[y, range(y.shape[0])] = 1\n",
    "\n",
    "    return y_one_hot\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bird': 2, 'deer': 4, 'frog': 6, 'truck': 9, 'dog': 5, 'cat': 3, 'horse': 7, 'ship': 8, 'automobile': 1, 'airplane': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdullah Mobeen\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'HW1-data/cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Fully-Connected Deep Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy after, 0 iteration =  9.375 %\n",
      "Training Set Cost after, 0 iteration =  2.3318684511732295\n",
      "Validation Set Accuracy after, 0 iteration =  8.94 %\n",
      "\n",
      "Training Set Accuracy after, 100 iteration =  25.78125 %\n",
      "Training Set Cost after, 100 iteration =  2.2041353010043707\n",
      "Validation Set Accuracy after, 100 iteration =  22.48 %\n",
      "\n",
      "Training Set Accuracy after, 200 iteration =  25.78125 %\n",
      "Training Set Cost after, 200 iteration =  2.113350889896978\n",
      "Validation Set Accuracy after, 200 iteration =  25.779999999999998 %\n",
      "\n",
      "Training Set Accuracy after, 300 iteration =  21.875 %\n",
      "Training Set Cost after, 300 iteration =  2.0871146684020676\n",
      "Validation Set Accuracy after, 300 iteration =  28.02 %\n",
      "\n",
      "Training Set Accuracy after, 400 iteration =  23.4375 %\n",
      "Training Set Cost after, 400 iteration =  2.07870720070934\n",
      "Validation Set Accuracy after, 400 iteration =  30.020000000000003 %\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-1784fff6061b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mNN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_dimensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'swish'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SGD'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#In the function call below, specify the hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.003\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-101-480a3e6464c5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, iters, alpha, batch_size, print_every)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[1;31m# compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m             \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m             \u001b[1;31m# update weights and biases based on gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-101-480a3e6464c5>\u001b[0m in \u001b[0;36mbackPropagation\u001b[1;34m(self, dAL, AL, cache)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m             \u001b[0mdA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maffineBackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m             \u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dW'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'db'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-101-480a3e6464c5>\u001b[0m in \u001b[0;36maffineBackward\u001b[1;34m(self, dA_prev, cache, layer)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msamples\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msamples\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mdA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 900, 600, 10]  # including the input and output layers\n",
    "#In the function call below, specify the activation (relu, lrelu, or swish) and optimization algorithm (Adam or SGD)\n",
    "NN = NeuralNetwork(layer_dimensions, 'swish', 'SGD')\n",
    "#In the function call below, specify the hyperparameters\n",
    "NN.train(X_train, y_train, iters=20000, alpha=0.003, batch_size=128, print_every=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 9 8 ... 3 2 7]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = NN.predict(X_test)\n",
    "print(y_predicted)\n",
    "save_predictions('ans1-uni', y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 9, 8, 5, 5, 1, 7, 4, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-uni.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regularized Neural Network with Dropout and L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy after, 0 iteration =  9.375 %\n",
      "Training Set Cost after, 0 iteration =  55.76793955767952\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-fe634c4bde62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mNN2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_dimensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lrelu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mNN2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.03\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-96-683566f1ec6b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, iters, alpha, batch_size, print_every)\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Set Cost after,\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"iteration = \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m                 \u001b[0mv_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m                 \u001b[0mval_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_pred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-683566f1ec6b>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[0mMake\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m         \"\"\"\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforwardPropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-683566f1ec6b>\u001b[0m in \u001b[0;36mforwardPropagation\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maffineForward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m             \u001b[0mA_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivationForward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-683566f1ec6b>\u001b[0m in \u001b[0;36maffineForward\u001b[1;34m(self, A, W, b)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NN2 = NeuralNetwork(layer_dimensions, 'lrelu', 'Adam',drop_prob=0.2, reg_lambda=0.05)\n",
    "NN2.train(X_train, y_train, iters=20000, alpha=0.03, batch_size=128, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 9 8 ... 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "y_predicted2 = NN2.predict(X_test)\n",
    "save_predictions('ans2-uni', y_predicted2)\n",
    "print(y_predicted2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
